# Install Dependencies
!pip install -q ultralytics pytesseract transformers torch torchvision requests
!apt-get install -y tesseract-ocr
!apt-get install -y libtesseract-dev

import cv2
import torch
import pytesseract
import requests
from pytesseract import image_to_string
from PIL import Image
from transformers import TableTransformerForObjectDetection, DetrImageProcessor

# Set up environment for Tesseract
pytesseract.pytesseract.tesseract_cmd = r'/usr/bin/tesseract'

# Load YOLOv10 Model
yolo_model = torch.hub.load('ultralytics/yolov5', 'custom', path='yolov10x_best.pt')

# LLaMA Vision API Base URL
LLAMA_API_URL = "https://llama-vision-api.com/analyze"  # Replace with actual API endpoint

def send_to_llama_vision(image_segment, task_type):
    """
    Send an image segment to LLaMA Vision for analysis.
    - task_type: "image" or "table"
    """
    _, img_encoded = cv2.imencode('.jpg', image_segment)
    files = {'file': ('segment.jpg', img_encoded.tobytes(), 'image/jpeg')}
    payload = {'task': task_type}

    try:
        response = requests.post(LLAMA_API_URL, files=files, data=payload)
        response.raise_for_status()
        return response.json()
    except requests.exceptions.RequestException as e:
        return f"LLaMA Vision Error: {str(e)}"

# Load Document Image (Upload file first)
from google.colab import files
uploaded = files.upload()

# Load Image
import io
import numpy as np

image_path = list(uploaded.keys())[0]
image = cv2.imdecode(np.frombuffer(uploaded[image_path], np.uint8), cv2.IMREAD_COLOR)

# Run YOLOv10 for Document Layout Detection
results = yolo_model(image)

# Display detection results
results.show()

# Load Table Model from Hugging Face
table_processor = DetrImageProcessor.from_pretrained("microsoft/table-transformer-detection")
table_model = TableTransformerForObjectDetection.from_pretrained("microsoft/table-transformer-detection")

def extract_text(image_segment):
    """Extract text using Tesseract OCR."""
    text = pytesseract.image_to_string(image_segment, lang='eng')
    return text.strip()

# Structured Results
doc_structure = []

# Iterate Over Detected Regions
for region in results.pandas().xyxy[0].itertuples():
    x1, y1, x2, y2, cls = int(region.xmin), int(region.ymin), int(region.xmax), int(region.ymax), region.name
    segment = image[y1:y2, x1:x2]

    entry = {"type": cls, "coordinates": (x1, y1, x2, y2)}

    if cls == "text":
        entry["content"] = extract_text(segment)
        
    elif cls == "image":
        entry["content"] = send_to_llama_vision(segment, "image")

    elif cls == "table":
        llama_table_result = send_to_llama_vision(segment, "table")
        
        if llama_table_result and "error" not in llama_table_result:
            entry["content"] = llama_table_result
        else:
            # Fallback to Hugging Face table model
            table_inputs = table_processor(images=Image.fromarray(segment), return_tensors="pt")
            outputs = table_model(**table_inputs)
            entry["content"] = "Table extracted with Hugging Face."

    doc_structure.append(entry)

# Display Extracted Content in Reading Order
for entry in doc_structure:
    print(f"Region Type: {entry['type']}")
    print(f"Content: {entry['content']}\n")
