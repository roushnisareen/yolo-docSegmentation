import cv2
import torch
import pytesseract
from PIL import Image
import numpy as np

# Ensure Tesseract is installed and path is set correctly
pytesseract.pytesseract.tesseract_cmd = r'C:\Program Files\Tesseract-OCR\tesseract.exe'

# Download YOLOv10x model weights if not available locally
model_path = "yolov10x_best.pt"
weights_url = "https://github.com/moured/YOLOv10-Document-Layout-Analysis/releases/download/doclaynet_weights/yolov10x_best.pt"
image_url = "https://raw.githubusercontent.com/moured/YOLOv10-Document-Layout-Analysis/main/images/input_sample.png"
image_path = "input_sample.png"

# Function to download file from URL if it doesn't exist
import requests
import os

def download_file(url, file_path):
    if not os.path.exists(file_path):
        print(f"Downloading {file_path}...")
        response = requests.get(url)
        with open(file_path, "wb") as file:
            file.write(response.content)
        print(f"Saved to {file_path}")

# Download model weights and sample image
download_file(weights_url, model_path)
download_file(image_url, image_path)

# Load the YOLO model using Torch Hub
print("Loading YOLOv10 model...")
model = torch.hub.load('ultralytics/yolov5', 'custom', path=model_path)

# Load the input image
image = cv2.imread(image_path)

# Perform inference on the image
results = model(image)
results.show()  # Show the detected regions

# Extract detected bounding boxes and labels
detections = results.xyxy[0].numpy()
for detection in detections:
    x_min, y_min, x_max, y_max, confidence, class_id = detection[:6]
    cv2.rectangle(image, (int(x_min), int(y_min)), (int(x_max), int(y_max)), (0, 255, 0), 2)

# Show the annotated image (optional)
cv2.imshow("Document Layout Analysis", image)
cv2.waitKey(0)
cv2.destroyAllWindows()

# Extract OCR text from detected regions
print("Extracting text from detected regions...")
extracted_text = ""
for detection in detections:
    x_min, y_min, x_max, y_max, confidence, class_id = detection[:6]
    roi = image[int(y_min):int(y_max), int(x_min):int(x_max)]  # Region of interest
    text = pytesseract.image_to_string(roi, lang='eng')
    extracted_text += f"Detected text: {text}\n"

print("Extracted Text:")
print(extracted_text)

# Save extracted text to a file
with open("extracted_text_from_analysis.txt", "w") as file:
    file.write(extracted_text)
